{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sentence Similarity with word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go\n",
    "\n",
    "we will use the moby-dick book from nltk gutenberg\n",
    "and a couple of test sentences\n",
    "\n",
    "We are building a classifier which classifies names into classes:\n",
    "    male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fileid):\n",
    "    \"\"\"\n",
    "        training a gensim model, see also: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    \"\"\"\n",
    "    return gensim.models.Word2Vec(gutenberg.sents(fileid), min_count=5, size=300, \n",
    "                                  workers=4, window=10, sg=1, negative=5, iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    feature_vec = np.zeros((300, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in sentence:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sent1, sent2, model, index2word_set):\n",
    "    s1_afv = avg_feature_vector(sent1, model=model, num_features=300, index2word_set=index2word_set)\n",
    "    s2_afv = avg_feature_vector(sent2, model=model, num_features=300, index2word_set=index2word_set)\n",
    "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "    return(sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_sents(target, sents, model, index2word_set):\n",
    "\n",
    "    print(\"Target sentence\", target)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range(len(sents)):\n",
    "        similarity = compute_similarity(target, sents[i], model, index2word_set)\n",
    "        res.append( (i, similarity) )\n",
    "\n",
    "    return sorted(res,key=itemgetter(1), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10059\n"
     ]
    }
   ],
   "source": [
    "fileid='melville-moby_dick.txt'\n",
    "raw_sents = gutenberg.sents(fileid)\n",
    "\n",
    "sents = []\n",
    "for s in raw_sents:\n",
    "    sent = [word for word in s if word.lower() not in stopwords.words('english')]\n",
    "    sents.append(sent)\n",
    "print(len(sents))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Starbuck', 0.8346410989761353), ('captain', 0.8299128413200378), ('Stubb', 0.829764723777771), ('mate', 0.8275545835494995), ('Peleg', 0.8258814811706543), ('Flask', 0.8169136643409729), ('chief', 0.8151313066482544), ('Captain', 0.8103663921356201), ('voice', 0.8071380853652954), ('Guernsey', 0.7917664051055908)]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(fileid)\n",
    "print(model.wv.most_similar(positive=['Ahab']))\n",
    "\n",
    "index2word_set = set(model.wv.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentence ['Ahab', 'boat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wohlg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2034, 0.9045610427856445), (5602, 0.9045560359954834), (9348, 0.8859259486198425), (8557, 0.8800023198127747), (7543, 0.8781915903091431), (7633, 0.8777551651000977), (8588, 0.8763377070426941), (3636, 0.8760122060775757), (9446, 0.8752614855766296), (8140, 0.8733166456222534)]\n",
      "\n",
      "['Meanwhile', 'Captain', 'Ahab', 'remained', 'invisibly', 'enshrined', 'within', 'cabin', '.'] (2034, 0.9045610427856445)\n",
      "['demanded', 'Ahab', ',', 'boat', 'drifted', 'back', '.'] (5602, 0.9045560359954834)\n",
      "['Ahab', 'turned', '.'] (9348, 0.8859259486198425)\n",
      "['cried', 'Starbuck', 'crew', ',', 'suddenly', 'admonished', 'vigilance', 'vivid', 'lightning', 'darting', 'flambeaux', ',', 'light', 'Ahab', 'post', '.'] (8557, 0.8800023198127747)\n",
      "['cried', 'Ahab', ',', 'suddenly', 'letting', 'suspended', 'breath', '.'] (7543, 0.8781915903091431)\n",
      "['back', 'stranger', 'ship', ',', 'face', 'set', 'like', 'flint', ',', 'Ahab', 'stood', 'upright', 'till', 'alongside', 'Pequod', '.'] (7633, 0.8777551651000977)\n",
      "['moment', 'Starbuck', 'caught', 'sight', 'Stubb', \"'\", 'face', 'slowly', 'beginning', 'glimmer', 'sight', '.'] (8588, 0.8763377070426941)\n",
      "['captain', 'Ahab', '.'] (3636, 0.8760122060775757)\n",
      "['cried', 'Ahab', ',', 'flattening', 'face', 'sky', '.'] (9446, 0.8752614855766296)\n",
      "['murmured', 'Ahab', ',', 'Starbuck', 'disappeared', '.'] (8140, 0.8733166456222534)\n"
     ]
    }
   ],
   "source": [
    "res = get_sim_sents(['Ahab', 'boat'], sents, model, index2word_set)\n",
    "print(res[:10])\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    print(sents[res[i][0]], res[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentence ['sperm', 'whale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wohlg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9530, 0.9259545803070068), (2640, 0.9251436591148376), (2711, 0.9109616279602051), (2637, 0.8996851444244385), (2667, 0.8916807174682617), (2735, 0.8815829157829285), (6825, 0.8801481127738953), (5888, 0.8799620270729065), (2552, 0.8761884570121765), (2164, 0.8756001591682434)]\n",
      "\n",
      "['*', 'motion', 'peculiar', 'sperm', 'whale', '.'] (9530, 0.9259545803070068)\n",
      "['right', 'whale', 'elsewhere', 'treated', 'length', ',', 'reference', 'elucidating', 'sperm', 'whale', '.'] (2640, 0.9251436591148376)\n",
      "['fishermen', 'approach', 'regarded', 'premonitory', 'advance', 'great', 'sperm', 'whale', '.'] (2711, 0.9109616279602051)\n",
      "['pretend', 'see', 'difference', 'Greenland', 'whale', 'English', 'right', 'whale', 'Americans', '.'] (2637, 0.8996851444244385)\n",
      "['Yet', 'seen', 'baleen', 'impossible', 'correctly', 'classify', 'Greenland', 'whale', '.'] (2667, 0.8916807174682617)\n",
      "['Narwhale', 'heard', 'called', 'Tusked', 'whale', ',', 'Horned', 'whale', ',', 'Unicorn', 'whale', '.'] (2735, 0.8815829157829285)\n",
      "['present', 'case', 'Erskine', 'contended', 'examples', 'whale', 'lady', 'reciprocally', 'illustrative', '.'] (6825, 0.8801481127738953)\n",
      "['respect', 'ears', ',', 'important', 'difference', 'observed', 'sperm', 'whale', 'right', '.'] (5888, 0.8799620270729065)\n",
      "['Scoresby', 'knew', 'nothing', 'says', 'nothing', 'great', 'sperm', 'whale', ',', 'compared', 'Greenland', 'whale', 'almost', 'unworthy', 'mentioning', '.'] (2552, 0.8761884570121765)\n",
      "['many', 'years', 'past', 'whale', '-', 'ship', 'pioneer', 'ferreting', 'remotest', 'least', 'known', 'parts', 'earth', '.'] (2164, 0.8756001591682434)\n"
     ]
    }
   ],
   "source": [
    "res = get_sim_sents(['sperm', 'whale'], sents, model, index2word_set)\n",
    "print(res[:10])\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    print(sents[res[i][0]], res[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
