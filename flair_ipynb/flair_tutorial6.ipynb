{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Creating a Corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading A Sequence Labeling Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "2019-04-29 18:39:07,359 Reading data from /home/wohlg/nltk_data/corpora/conll2000\n",
      "2019-04-29 18:39:07,360 Train: /home/wohlg/nltk_data/corpora/conll2000/train.txt\n",
      "2019-04-29 18:39:07,361 Dev: None\n",
      "2019-04-29 18:39:07,362 Test: /home/wohlg/nltk_data/corpora/conll2000/test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'chunk'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/home/wohlg/nltk_data/corpora/conll2000/'\n",
    "\n",
    "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
    "                                                              train_file='train.txt',\n",
    "                                                              test_file='test.txt')\n",
    "                                                              # dev_file='dev.txt')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have TaggedCorpus object that contains the train, and test splits, each has a *list of Sentence*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8042\n",
      "2012\n",
      "Confidence <NN> in <IN> the <DT> pound <NN> is <VBZ> widely <RB> expected <VBN> to <TO> take <VB> another <DT> sharp <JJ> dive <NN> if <IN> trade <NN> figures <NNS> for <IN> September <NNP> , <,> due <JJ> for <IN> release <NN> tomorrow <NN> , <,> fail <VB> to <TO> show <VB> a <DT> substantial <JJ> improvement <NN> from <IN> July <NNP> and <CC> August <NNP> 's <POS> near-record <JJ> deficits <NNS> . <.>\n",
      "Confidence <B-NP> in <B-PP> the <B-NP> pound <I-NP> is <B-VP> widely <I-VP> expected <I-VP> to <I-VP> take <I-VP> another <B-NP> sharp <I-NP> dive <I-NP> if <B-SBAR> trade <B-NP> figures <I-NP> for <B-PP> September <B-NP> , due <B-ADJP> for <B-PP> release <B-NP> tomorrow <B-NP> , fail <B-VP> to <I-VP> show <I-VP> a <B-NP> substantial <I-NP> improvement <I-NP> from <B-PP> July <B-NP> and <I-NP> August <I-NP> 's <B-NP> near-record <I-NP> deficits <I-NP> .\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus.train))\n",
    "print(len(corpus.test))\n",
    "\n",
    "print(corpus.train[0].to_tagged_string('pos'))\n",
    "print(corpus.train[0].to_tagged_string('chunk'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Text Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-29 18:47:47,062 Reading data from /home/wohlg/itmo/misc/cooking_classification/simple_and_preprocessed\n",
      "2019-04-29 18:47:47,063 Train: /home/wohlg/itmo/misc/cooking_classification/simple_and_preprocessed/cooking.train\n",
      "2019-04-29 18:47:47,064 Dev: /home/wohlg/itmo/misc/cooking_classification/simple_and_preprocessed/cooking.valid\n",
      "2019-04-29 18:47:47,065 Test: /home/wohlg/itmo/misc/cooking_classification/simple_and_preprocessed/cooking.test\n",
      "Done loading\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 7502,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"sauce\": 327,\n",
      "            \"cheese\": 227,\n",
      "            \"food-safety\": 943,\n",
      "            \"storage-method\": 359,\n",
      "            \"equipment\": 649,\n",
      "            \"bread\": 564,\n",
      "            \"baking\": 1133,\n",
      "            \"substitutions\": 710,\n",
      "            \"chocolate\": 227,\n",
      "            \"oven\": 223,\n",
      "            \"storage-lifetime\": 252,\n",
      "            \"cake\": 309,\n",
      "            \"flavor\": 290,\n",
      "            \"beef\": 190,\n",
      "            \"food-science\": 220,\n",
      "            \"cookies\": 179,\n",
      "            \"fruit\": 211,\n",
      "            \"vegetables\": 243,\n",
      "            \"meat\": 327,\n",
      "            \"oil\": 224,\n",
      "            \"chicken\": 369,\n",
      "            \"eggs\": 344,\n",
      "            \"pasta\": 184,\n",
      "            \"frying\": 227,\n",
      "            \"temperature\": 211,\n",
      "            \"sugar\": 206,\n",
      "            \"food-preservation\": 192,\n",
      "            \"freezing\": 281,\n",
      "            \"coffee\": 229,\n",
      "            \"dough\": 224\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 74593,\n",
      "            \"min\": 2,\n",
      "            \"max\": 37,\n",
      "            \"avg\": 9.943081844841375\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 1000,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"food-safety\": 124,\n",
      "            \"beef\": 31,\n",
      "            \"chicken\": 62,\n",
      "            \"baking\": 146,\n",
      "            \"equipment\": 85,\n",
      "            \"dough\": 29,\n",
      "            \"vegetables\": 33,\n",
      "            \"pasta\": 29,\n",
      "            \"sauce\": 44,\n",
      "            \"eggs\": 41,\n",
      "            \"cookies\": 32,\n",
      "            \"bread\": 56,\n",
      "            \"substitutions\": 107,\n",
      "            \"food-science\": 20,\n",
      "            \"food-preservation\": 24,\n",
      "            \"meat\": 46,\n",
      "            \"temperature\": 33,\n",
      "            \"flavor\": 51,\n",
      "            \"storage-method\": 36,\n",
      "            \"chocolate\": 32,\n",
      "            \"coffee\": 29,\n",
      "            \"freezing\": 33,\n",
      "            \"storage-lifetime\": 24,\n",
      "            \"cheese\": 37,\n",
      "            \"fruit\": 16,\n",
      "            \"oil\": 24,\n",
      "            \"sugar\": 20,\n",
      "            \"cake\": 48,\n",
      "            \"oven\": 34,\n",
      "            \"frying\": 25\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 9850,\n",
      "            \"min\": 2,\n",
      "            \"max\": 31,\n",
      "            \"avg\": 9.85\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 1000,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"substitutions\": 103,\n",
      "            \"frying\": 28,\n",
      "            \"flavor\": 37,\n",
      "            \"coffee\": 28,\n",
      "            \"eggs\": 45,\n",
      "            \"beef\": 21,\n",
      "            \"temperature\": 29,\n",
      "            \"sauce\": 30,\n",
      "            \"freezing\": 27,\n",
      "            \"baking\": 165,\n",
      "            \"oil\": 34,\n",
      "            \"chocolate\": 31,\n",
      "            \"meat\": 43,\n",
      "            \"food-safety\": 144,\n",
      "            \"storage-lifetime\": 36,\n",
      "            \"vegetables\": 28,\n",
      "            \"equipment\": 82,\n",
      "            \"cheese\": 35,\n",
      "            \"cookies\": 36,\n",
      "            \"bread\": 67,\n",
      "            \"pasta\": 26,\n",
      "            \"chicken\": 57,\n",
      "            \"food-science\": 23,\n",
      "            \"sugar\": 28,\n",
      "            \"storage-method\": 51,\n",
      "            \"food-preservation\": 18,\n",
      "            \"oven\": 33,\n",
      "            \"dough\": 26,\n",
      "            \"fruit\": 22,\n",
      "            \"cake\": 45\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 10011,\n",
      "            \"min\": 2,\n",
      "            \"max\": 31,\n",
      "            \"avg\": 10.011\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from pathlib import Path\n",
    "\n",
    "# use your own data path\n",
    "data_folder = Path('/home/wohlg/itmo/misc/cooking_classification/simple_and_preprocessed')\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
    "                                                                     test_file='cooking.test',\n",
    "                                                                     dev_file='cooking.valid',\n",
    "                                                                     train_file='cooking.train')\n",
    "    \n",
    "print('Done loading')\n",
    "print(corpus.obtain_statistics())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and use a builtin corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-29 18:57:23,394 Reading data from /home/wohlg/.flair/datasets/imdb\n",
      "2019-04-29 18:57:23,397 Train: /home/wohlg/.flair/datasets/imdb/train.txt\n",
      "2019-04-29 18:57:23,399 Dev: None\n",
      "2019-04-29 18:57:23,404 Test: /home/wohlg/.flair/datasets/imdb/test.txt\n",
      "TaggedCorpus: 2250 train + 250 dev + 2500 test sentences\n"
     ]
    }
   ],
   "source": [
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB).downsample(0.1)\n",
    "print(TaggedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus from one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading included corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "2019-04-30 08:17:08,462 Reading data from /home/wohlg/.flair/datasets/ud_english\n",
      "2019-04-30 08:17:08,462 Train: /home/wohlg/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2019-04-30 08:17:08,463 Dev: /home/wohlg/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2019-04-30 08:17:08,463 Test: /home/wohlg/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 12543,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 204585,\n",
      "            \"min\": 1,\n",
      "            \"max\": 159,\n",
      "            \"avg\": 16.310691222195647\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 2077,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 25096,\n",
      "            \"min\": 1,\n",
      "            \"max\": 81,\n",
      "            \"avg\": 12.082811747713048\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 2002,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 25148,\n",
      "            \"min\": 1,\n",
      "            \"max\": 75,\n",
      "            \"avg\": 12.561438561438562\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.obtain_statistics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 3763,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 61173,\n",
      "            \"min\": 1,\n",
      "            \"max\": 135,\n",
      "            \"avg\": 16.25644432633537\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 624,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 7759,\n",
      "            \"min\": 1,\n",
      "            \"max\": 75,\n",
      "            \"avg\": 12.434294871794872\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 601,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 7708,\n",
      "            \"min\": 1,\n",
      "            \"max\": 65,\n",
      "            \"avg\": 12.825291181364392\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus.downsample(0.3)\n",
    "print(corpus.obtain_statistics())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
